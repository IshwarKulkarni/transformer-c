{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "def my_assert(a, b, eps=1e-6):\n",
    "    if (a.shape != b.shape):\n",
    "        print(\"shapes mismatch: a.shape\", a.shape, 'b.shape: ',  b.shape)\n",
    "        assert(False)\n",
    "    if(not torch.allclose(a, b, eps)):\n",
    "        print(\"\\na:\\n\", a, \"\\nb:\\n\", b, \"\\na/b:\\n\", a/b)\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test attention, test_attention\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(551)\n",
    "torch.set_printoptions(precision=8, linewidth=2000)\n",
    "\n",
    "bn = 4\n",
    "x0w = 13 # input embedding size\n",
    "Eq = 15  # query embedding size\n",
    "Ek = Eq # key embedding size\n",
    "Ev = x0w # value , i.e. output embedding size\n",
    "S = 16 # seq_len\n",
    "\n",
    "Q = torch.nn.Parameter(torch.randn(Eq, x0w) * math.sqrt(2.0/(x0w + Eq)))\n",
    "K = torch.nn.Parameter(torch.randn(Ek, x0w) * math.sqrt(2.0/(x0w + Ek)))\n",
    "V = torch.nn.Parameter(torch.randn(Ev, x0w) * math.sqrt(2.0/(x0w + Ev)))\n",
    "\n",
    "q = torch.rand(bn, S, x0w)\n",
    "k = torch.rand(bn, S, x0w)\n",
    "v = torch.rand(bn, S, x0w)\n",
    "\n",
    "q_ = q @ Q.t()\n",
    "k_ = k @ K.t()\n",
    "v_ = v @ V.t()\n",
    "\n",
    "q_.retain_grad()\n",
    "k_.retain_grad()\n",
    "v_.retain_grad()\n",
    "\n",
    "qkt = q_ @ k_.transpose(1, 2) / (Eq ** .5)\n",
    "smax = torch.softmax(qkt, dim=-1)\n",
    "output = smax @ v_\n",
    "\n",
    "qkt.retain_grad()\n",
    "smax.retain_grad()\n",
    "output.retain_grad()\n",
    "\n",
    "target = torch.randn(output.shape) * 2 + 1\n",
    "target = torch.ones(target.shape)\n",
    "e = (target - output).pow(2).mean()\n",
    "e.backward()\n",
    "\n",
    "# qkt_grad in terms of q_ and k_\n",
    "\n",
    "filename = \"static_data/attention.txt\"\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"{bn} {x0w} {Eq} {Ek} {Ev} {S} {e.item()}\\n\")\n",
    "\n",
    "l2_grad = 2 * (output - target) / (output.numel())\n",
    "\n",
    "qkt_grad = l2_grad @ v_.transpose(1, 2)\n",
    "\n",
    "q_grad_in = (qkt.grad @ k_) / (Eq ** .5)\n",
    "k_grad_in = (qkt.grad.transpose(1, 2) @ q_) / (Eq ** .5)\n",
    "v_grad_in = smax.transpose(1, 2) @ l2_grad\n",
    "\n",
    "#print(\"qkt_grad:\\n\", qkt_grad)\n",
    "#print(\"q_grad_in:\\n\" , q_grad_in)\n",
    "#print(\"k_grad_in:\\n\" , k_grad_in)\n",
    "\n",
    "Q_grad_in = (q_grad_in.transpose(1, 2) @ q)\n",
    "K_grad_in = (k_grad_in.transpose(1, 2) @ k)\n",
    "V_grad_in = (v_grad_in.transpose(1, 2) @ v)\n",
    "\n",
    "my_assert(q_.grad, q_grad_in)\n",
    "my_assert(k_.grad, k_grad_in)\n",
    "my_assert(v_.grad, v_grad_in)\n",
    "\n",
    "print(Q_grad_in)\n",
    "\n",
    "my_assert(Q.grad, Q_grad_in.sum(0))\n",
    "my_assert(K.grad, K_grad_in.sum(0))\n",
    "my_assert(V.grad, V_grad_in.sum(0))\n",
    "\n",
    "for target in [Q, K, V, q, k, v, target, qkt, smax, output, q_, k_, v_, Q.grad, K.grad, V.grad]:\n",
    "    save_tensor_to_csv(target, filename, True)\n",
    "#print(Q.grad.shape, '\\n', Q.grad)\n",
    "#print(K.grad.shape, '\\n', K.grad)\n",
    "#print(V.grad.shape, '\\n', V.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test test_linearb\n",
    "import torch\n",
    "import os\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(501)\n",
    "torch.set_printoptions(precision=8, linewidth=2000, sci_mode=False)\n",
    "bn = 12\n",
    "x0w = 17\n",
    "Sl = 14\n",
    "I0 = 15\n",
    "I2 = 16\n",
    "\n",
    "x0 = torch.randn(bn, Sl, x0w)\n",
    "W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "#b1 = torch.nn.Parameter(torch.randn(1, I0))\n",
    "W1 = torch.nn.Parameter(torch.randn(I2, I0))\n",
    "b1 = torch.nn.Parameter(torch.randn(1, I2))\n",
    "\n",
    "z1 = x0 @ W0.t()\n",
    "y1 = z1.sigmoid()\n",
    "z2 = (y1 @ W1.t() + b1)\n",
    "#y2 = z2.sigmoid()\n",
    "\n",
    "#y2.retain_grad()\n",
    "z2.retain_grad()\n",
    "y1.retain_grad()\n",
    "z1.retain_grad()\n",
    "\n",
    "target = torch.randn(z2.shape)\n",
    "#e = (-t * sm.log()).mean()\n",
    "e = (target - z2).pow(2).mean()\n",
    "e.backward()\n",
    "\n",
    "filename = f\"static_data/linear.txt\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(filename, \"a\") as f:\n",
    "    f.write(f\"{bn} {x0w} {Sl} {I0} {I2} {e.item()}\\n\")\n",
    "\n",
    "def sigmoid_backward(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "z2_grad = 2 * (z2 - target) / z2.numel()  # gradientIn for Linear::backward()\n",
    "my_assert(z2.grad, z2_grad)\n",
    "\n",
    "y1_grad = z2_grad @ W1\n",
    "my_assert(y1.grad, y1_grad)\n",
    "\n",
    "w2_grad = z2_grad.transpose(1, 2) @ y1\n",
    "my_assert(W1.grad, w2_grad.sum(0))\n",
    "\n",
    "b2_grad = z2_grad.sum(dim=1, keepdim=True)\n",
    "my_assert(b1.grad, b2_grad.sum(0))\n",
    "\n",
    "#####\n",
    "z1_grad = y1_grad * sigmoid_backward(y1)\n",
    "my_assert(z1.grad, z1_grad)\n",
    "\n",
    "w1_grad = z1_grad.transpose(1, 2) @ x0\n",
    "my_assert(W0.grad, w1_grad.sum(0))\n",
    "\n",
    "save_tensor_to_csv(x0, filename, True)\n",
    "save_tensor_to_csv(target, filename, True)\n",
    "\n",
    "save_tensor_to_csv(W0, filename, True)\n",
    "save_tensor_to_csv(W1, filename, True)\n",
    "save_tensor_to_csv(b1, filename, True)\n",
    "\n",
    "save_tensor_to_csv(z2, filename, True) # output of first layer\n",
    "save_tensor_to_csv(y1, filename, True) # output of second layer\n",
    "\n",
    "### z2_grad = y2_grad * sigmoid_backward(y2)\n",
    "save_tensor_to_csv(z2.grad, filename, True) #loss.gradOut; \n",
    "save_tensor_to_csv(y1.grad, filename, True) #y2.gradOut; z2_grad @ W2 , \n",
    "\n",
    "save_tensor_to_csv(W1.grad, filename, True) # z2_grad.transpose(1, 2) @ y1\n",
    "save_tensor_to_csv(b1.grad, filename, True) # z2_grad.sum(dim=1, keepdim=True)\n",
    "\n",
    "# z1_grad = y1_grad * sigmoid_backward(y1)\n",
    "save_tensor_to_csv(W0.grad, filename, True) # z1_grad.transpose(1, 2) @ x0\n",
    "#save_tensor_to_csv(b1.grad, filename, True) # z1_grad.sum(dim=0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test softmax cross entropy, test_LSMCELoss*\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(10)\n",
    "torch.set_printoptions(edgeitems=2000, linewidth=200, sci_mode=False, threshold=2000, precision=10, profile=\"full\")\n",
    "\n",
    "x = torch.randn(3, 5, 3)\n",
    "W0 = torch.nn.Parameter(torch.randn(5, 3))\n",
    "b0 = torch.nn.Parameter(torch.randn(1, 5))\n",
    "\n",
    "L = x @ W0.t() + b0\n",
    "L.retain_grad()\n",
    "\n",
    "target = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0]\n",
    "    ]\n",
    ").float()\n",
    "target = [target] * x.shape[0]\n",
    "target = torch.stack(target)\n",
    "\n",
    "o = -target * F.log_softmax(L, dim=-1)\n",
    "e = o.sum(-1).mean()\n",
    "\n",
    "e.retain_grad()\n",
    "e.backward()\n",
    "\n",
    "filename = f\"static_data/lsmce.txt\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "save_tensor_to_csv(x, filename, True)\n",
    "save_tensor_to_csv(W0, filename, True)\n",
    "save_tensor_to_csv(b0, filename, True)\n",
    "save_tensor_to_csv(target, filename, True)\n",
    "\n",
    "with open(filename, \"a\") as f:\n",
    "    f.write(f\"{e.item()}\\n\")\n",
    "\n",
    "save_tensor_to_csv(L, filename, True)\n",
    "save_tensor_to_csv(L.grad, filename, True) #loss.gradOut;\n",
    "\n",
    "save_tensor_to_csv(W0.grad, filename, True)\n",
    "save_tensor_to_csv(b0.grad, filename, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test softmax dimN , testing test_softmaxDim N = [0,1]\n",
    "\n",
    "import torch\n",
    "import os \n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.set_printoptions(precision=8, linewidth=2000, sci_mode=False)\n",
    "torch.manual_seed(1331)\n",
    "\n",
    "bn = 2\n",
    "x0w = 68\n",
    "Sl = 33\n",
    "I0 = 35\n",
    "def write_softmax(N = 1):\n",
    "    \n",
    "    x0 = torch.randn(bn, Sl, x0w)\n",
    "    W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "    b0 = torch.nn.Parameter(torch.randn(1, I0))\n",
    "    z1 = (x0 @ W0.t() + b0).tanh()\n",
    "    s = torch.softmax(z1, dim=(-1-N))    \n",
    "    t = torch.randn(s.shape)\n",
    "    e = (-t * s.log()).mean()\n",
    "    \n",
    "    z1.retain_grad()\n",
    "    s.retain_grad()\n",
    "    e.retain_grad()\n",
    "    e.backward()\n",
    "    filename = f\"static_data/sm_dim{N}.txt\"\n",
    "\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"{bn} {x0w} {Sl} {I0} {e.item()}\\n\")\n",
    "\n",
    "    for t in [x0, W0, b0, t,z1, s, z1.grad, W0.grad, b0.grad]:\n",
    "        save_tensor_to_csv(t, filename, True)\n",
    "    \n",
    "    print(\"z1.grad.abs().sum(): \", z1.grad.abs().sum())\n",
    "    print(\"W0.grad.abs().sum(): \", W0.grad.abs().sum())\n",
    "    print(\"b0.grad.abs().sum(): \", b0.grad.abs().sum(), \"\\n\")\n",
    "\n",
    "write_softmax(0)\n",
    "write_softmax(1)\n",
    "\n",
    "# print magnitude of gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_average_node, mean along dim=-1, -2 etc.\n",
    "import torch\n",
    "import os\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(999)\n",
    "torch.set_printoptions(precision=8, linewidth=2000, sci_mode=False)\n",
    "\n",
    "bn = 3\n",
    "x0w = 7\n",
    "Sl = 4\n",
    "I0 = 5\n",
    "\n",
    "x0 = torch.randn(bn, Sl, x0w)\n",
    "W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "b0 = torch.nn.Parameter(torch.randn(1, I0))\n",
    "z1 = (x0 @ W0.t() + b0).tanh()\n",
    "\n",
    "y1 = z1.mean(dim=1, keepdim=True)\n",
    "y1.retain_grad()\n",
    "target = torch.randn(y1.shape)\n",
    "e = (target - y1).pow(2).mean()\n",
    "e.backward()\n",
    "\n",
    "try:\n",
    "    os.remove(\"static_data/average.txt\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "filename = f\"static_data/average.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"{bn} {x0w} {Sl} {I0} {e.item()}\\n\")\n",
    "\n",
    "for t in [x0, W0, b0, target, z1, y1, W0.grad, b0.grad]:\n",
    "    save_tensor_to_csv(t, filename, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer norm\n",
    "import torch\n",
    "import os\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(999)\n",
    "torch.set_printoptions(precision=8, linewidth=2000, sci_mode=False)\n",
    "\n",
    "bn = 2\n",
    "x0w = 5\n",
    "Sl = 3\n",
    "I0 = 6\n",
    "\n",
    "x0 = torch.randn(bn, Sl, x0w) + 0.5\n",
    "W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "b0 = torch.nn.Parameter(torch.randn(1, I0))\n",
    "y = (x0 @ W0.t() + b0).sigmoid()\n",
    "\n",
    "norm = torch.nn.LayerNorm(y.shape[-1])\n",
    "#norm = torch.nn.BatchNorm1d(y.shape[0])\n",
    "#print(y.permute(0, 2, 1).shape)\n",
    "\n",
    "z = norm(y)\n",
    "z.retain_grad()\n",
    "y.retain_grad()\n",
    "\n",
    "y_mean = y.mean(dim=-1, keepdim=True)\n",
    "y_sq = y ** 2\n",
    "y_sq_mean = y_sq.mean(dim=-1, keepdim=True)\n",
    "\n",
    "y_var = y_sq_mean - y_mean ** 2\n",
    "y_num = y - y_mean\n",
    "y_std = y_var ** 0.5\n",
    "y_norm = y_num/y_std\n",
    "\n",
    "target = torch.randn(z.shape)\n",
    "e = (target - y_norm).pow(2).mean()\n",
    "\n",
    "e.backward()\n",
    "\n",
    "my_assert(y_norm, z, 1e-3)\n",
    "\n",
    "filename = \"static_data/layer_norm.txt\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"{bn} {x0w} {Sl} {I0} {e.item()}\\n\")\n",
    "\n",
    "for t in [x0, W0, b0, target, y, z, W0.grad, b0.grad]:\n",
    "    save_tensor_to_csv(t, filename, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test product, see test_productT in tests.cpp\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "torch.manual_seed(509)\n",
    "torch.set_printoptions(precision=8, linewidth=2000, sci_mode=False)\n",
    "\n",
    "bn = 5\n",
    "x0w = 17\n",
    "Sl = 14\n",
    "I0 = 15\n",
    "I2 = 16\n",
    "I3 = 17\n",
    "\n",
    "x0 = torch.randn(bn, Sl, x0w)\n",
    "W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "b0 = torch.nn.Parameter(torch.randn(1, I0))\n",
    "\n",
    "x1 = torch.randn(bn, I3, I2)\n",
    "W1 = torch.nn.Parameter(torch.randn(I0, I2))\n",
    "\n",
    "z0 = x0 @ W0.t() + b0\n",
    "y0 = z0.sigmoid()\n",
    "\n",
    "y1 = x1 @ W1.t()\n",
    "A = y0 @ y1.transpose(1, 2) / 2.222\n",
    "\n",
    "t = torch.randn(A.shape)\n",
    "e = (t - A).pow(2).mean()\n",
    "\n",
    "A.retain_grad()\n",
    "y1.retain_grad()\n",
    "y0.retain_grad()\n",
    "z0.retain_grad()\n",
    "\n",
    "e.backward()\n",
    "\n",
    "filename = f\"static_data/productT.txt\"\n",
    "try:\n",
    "    os.remove(filename) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"{bn} {x0w} {Sl} {I0} {I2} {I3} {e.item()}\\n\")\n",
    "\n",
    "for tensor in [x0, W0, b0, x1, W1, t, A]:\n",
    "    save_tensor_to_csv(tensor, filename, True)\n",
    "    \n",
    "for tensor in [y1, y0]:\n",
    "    save_tensor_to_csv(tensor, filename, True)\n",
    "\n",
    "for tensor in [A.grad, W0.grad, b0.grad, W1.grad]:\n",
    "    save_tensor_to_csv(tensor, filename, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division grad\n",
    "import torch\n",
    "torch.manual_seed(509)\n",
    "\n",
    "bn = 5\n",
    "x0w = 17\n",
    "Sl = 14\n",
    "I0 = 15\n",
    "\n",
    "x0 = torch.randn(bn, Sl, x0w)\n",
    "W0 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "\n",
    "x1 = torch.randn(bn, Sl, x0w)\n",
    "W1 = torch.nn.Parameter(torch.randn(I0, x0w))\n",
    "\n",
    "z0 = x0 @ W0.t()\n",
    "y0 = z0.sigmoid()\n",
    "\n",
    "y1 = x1 @ W1.t()\n",
    "y1 = y1.sigmoid()\n",
    "out = y0 / y1\n",
    "t = torch.randn(out.shape)\n",
    "e = (t - out).pow(2).mean()\n",
    "\n",
    "out.retain_grad()\n",
    "y1.retain_grad()\n",
    "y0.retain_grad()\n",
    "z0.retain_grad()\n",
    "\n",
    "e.backward()\n",
    "\n",
    "diff_grad = 2 * (out - t) / out.numel()\n",
    "my_assert(out.grad, diff_grad)\n",
    "\n",
    "my_assert(y0.grad, diff_grad / y1)\n",
    "my_assert(y1.grad, -diff_grad * y0 / (y1 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.functional' has no attribute 'layer_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m nx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m(x, [\u001b[38;5;241m3\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, nx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, nx\u001b[38;5;241m/\u001b[39mx)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.functional' has no attribute 'layer_norm'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.randn(2, 2, 3)\n",
    "nx = torch.functional.layer_norm(x, [3], None, None, 1e-5)\n",
    "print(x, '\\n', nx, '\\n', nx/x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
