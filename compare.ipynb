{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fc \n",
    "import torch \n",
    "xh = 3\n",
    "xw = 4\n",
    "th = 2\n",
    "inner = 5\n",
    "torch.set_printoptions(precision=10, linewidth=2000)\n",
    "torch.random.manual_seed(20)\n",
    "W0 = torch.nn.Parameter(torch.randn(inner, xh) )\n",
    "b0 = torch.nn.Parameter(torch.randn(inner, 1) )\n",
    "W1 = torch.nn.Parameter(torch.randn(th, inner) )\n",
    "b1 = torch.nn.Parameter(torch.randn(th, 1) )\n",
    "\n",
    "x = torch.ones(xh, xw)\n",
    "y1 = torch.mm(W0, x) + b0\n",
    "z1 = torch.sigmoid(y1)\n",
    "y2 = torch.mm(W1, z1) + b1\n",
    "z2 = torch.sigmoid(y2)\n",
    "\n",
    "print(\"W0.shape\", W0.shape, \"\\n values: \", W0)\n",
    "print(\"b0.shape\", b0.shape, \"\\n values: \", b0)\n",
    "print(\"W1.shape\", W1.shape, \"\\n values: \", W1)\n",
    "print(\"b1.shape\", b1.shape, \"\\n values: \", b1)\n",
    "\n",
    "exp = torch.exp(z2)\n",
    "s = exp / exp.sum(dim=0, keepdim=True)\n",
    "\n",
    "# cross entropy:\n",
    "t = torch.ones_like(s) * 5\n",
    "#print(\"t: \", t, \"\\ns: \", s)\n",
    "ce = -(t * torch.log(s)).sum()\n",
    "#ce = (s - t).pow(2).mean()\n",
    "ce.backward()\n",
    "print(\"error: \" , ce)\n",
    "\n",
    "print(\"W0.grad.shape\", W0.grad.shape, \"\\nW0.grad:\" , W0.grad)\n",
    "print(\"b0.grad.shape\", b0.grad.shape, \"\\nb0.grad:\" , b0.grad)\n",
    "print(\"W1.grad.shape\", W1.grad.shape, \"\\nW1.grad:\" , W1.grad)\n",
    "print(\"b1.grad.shape\", b1.grad.shape, \"\\nb1.grad:\" , b1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test attention\n",
    "import torch\n",
    "from tests.datagen import save_tensor_to_csv\n",
    "\n",
    "embed_size = 7\n",
    "seq_len = 5\n",
    "torch.set_printoptions(precision=8, linewidth=2000)\n",
    "torch.random.manual_seed(10)\n",
    "\n",
    "Q = torch.nn.Parameter(torch.randn(seq_len, embed_size))\n",
    "K = torch.nn.Parameter(torch.randn(seq_len, embed_size))\n",
    "V = torch.nn.Parameter(torch.randn(seq_len, embed_size))\n",
    "\n",
    "#print(\"Q: \", Q)\n",
    "#print(\"K: \", K)\n",
    "#print(\"V: \", V)\n",
    "\n",
    "save_tensor_to_csv(Q, \"data/attention_q.csv\")\n",
    "save_tensor_to_csv(K, \"data/attention_k.csv\")\n",
    "save_tensor_to_csv(V, \"data/attention_v.csv\")\n",
    "\n",
    "x = torch.ones(embed_size, seq_len)\n",
    "\n",
    "q_ = x * .1 #torch.mm(Lq, x.t())\n",
    "k_ = x * .2 #torch.mm(Lk, x.t())\n",
    "v_ = x * .3 #torch.mm(Lv, x.t())\n",
    "\n",
    "q = Q @ q_\n",
    "k = K @ k_\n",
    "v = V @ v_\n",
    "v.retain_grad()\n",
    "k.retain_grad()\n",
    "q.retain_grad()\n",
    "#print(\"q: \", q)\n",
    "#print(\"k: \", k)\n",
    "#print(\"v: \", v)\n",
    "\n",
    "qkt = q @ k.t() / (embed_size ** (1 / 2))\n",
    "qkt.retain_grad()\n",
    "#print(\"qkt: \", qkt)\n",
    "\n",
    "s = torch.softmax(qkt, dim=-1)\n",
    "#s = qkt.exp() / qkt.exp().sum(dim=1, keepdim=True)\n",
    "s.retain_grad()\n",
    "#print(\"s: \", s)\n",
    "\n",
    "att = s @ v\n",
    "att.retain_grad()\n",
    "#print(\"att\", att)\n",
    "\n",
    "err = (torch.ones_like(att) - att).pow(2).mean()\n",
    "err.retain_grad()\n",
    "err.backward()\n",
    "print(err)\n",
    "\n",
    "mse_grad = 2 * (att - torch.ones(seq_len, seq_len))/( seq_len * seq_len)\n",
    "\n",
    "assert(torch.allclose(att.grad, mse_grad))\n",
    "v_grad_in = torch.mm(s.t(), mse_grad)\n",
    "assert(torch.allclose(v.grad, v_grad_in))\n",
    "#print(\"v.grad:\\n\", v.grad)\n",
    "\n",
    "s_grad_in = torch.mm(mse_grad, v.t())\n",
    "assert(torch.allclose(s.grad, s_grad_in))\n",
    "#print(\"s.grad:\\n\", s.grad)\n",
    "\n",
    "V_grad = torch.mm(v_grad_in, v_.t())\n",
    "assert(torch.allclose(V.grad, V_grad))\n",
    "#print(\"V.grad:\\n\", V.grad)\n",
    "\n",
    "#print(\"qkt.grad == s_grad_out in C++\\n\", qkt.grad)\n",
    "\n",
    "torch.set_printoptions(precision=8)\n",
    "k_grad_in = torch.mm(q.t(), qkt.grad).t() / (embed_size ** (1 / 2))\n",
    "allclose = torch.allclose(k_grad_in, k.grad)\n",
    "#print(\"k_grad_in:\\n\",k_grad_in)\n",
    "#print(\"K.grad:\\n\", K.grad)\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=10)\n",
    "q_grad_in = torch.mm(qkt.grad, k) / (embed_size ** (1 / 2))\n",
    "#print(\"q_grad_in:\\n\", q_grad_in)\n",
    "\n",
    "\n",
    "print(\"att.grad:\\n\", att.grad)\n",
    "print(\"s.grad:\\n\", s.grad)\n",
    "print(\"V.grad:\\n\", v.grad)\n",
    "print(\"qkt.grad:\\n\", qkt.grad)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
